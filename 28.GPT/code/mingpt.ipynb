{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport sys\nimport json\nimport random\nfrom ast import literal_eval\n\nimport numpy as np\nimport torch\n\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef setup_logging(config):\n    \"\"\" monotonous bookkeeping \"\"\"\n    work_dir = config.system.work_dir\n    # create the work directory if it doesn't already exist\n    os.makedirs(work_dir, exist_ok=True)\n    # log the args (if any)\n    with open(os.path.join(work_dir, 'args.txt'), 'w') as f:\n        f.write(' '.join(sys.argv))\n    # log the config itself\n    with open(os.path.join(work_dir, 'config.json'), 'w') as f:\n        f.write(json.dumps(config.to_dict(), indent=4))\n\nclass CN:\n    \"\"\" a lightweight configuration class inspired by yacs \"\"\"\n    # TODO: convert to subclass from a dict like in yacs?\n    # TODO: implement freezing to prevent shooting of own foot\n    # TODO: additional existence/override checks when reading/writing params?\n\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\n    def __str__(self):\n        return self._str_helper(0)\n\n    def _str_helper(self, indent):\n        \"\"\" need to have a helper to support nested indentation for pretty printing \"\"\"\n        parts = []\n        for k, v in self.__dict__.items():\n            if isinstance(v, CfgNode):\n                parts.append(\"%s:\\n\" % k)\n                parts.append(v._str_helper(indent + 1))\n            else:\n                parts.append(\"%s: %s\\n\" % (k, v))\n        parts = [' ' * (indent * 4) + p for p in parts]\n        return \"\".join(parts)\n\n    def to_dict(self):\n        \"\"\" return a dict representation of the config \"\"\"\n        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n\n    def merge_from_dict(self, d):\n        self.__dict__.update(d)\n\n    def merge_from_args(self, args):\n        \"\"\"\n        update the configuration from a list of strings that is expected\n        to come from the command line, i.e. sys.argv[1:].\n\n        The arguments are expected to be in the form of `--arg=value`, and\n        the arg can use . to denote nested sub-attributes. Example:\n\n        --model.n_layer=10 --trainer.batch_size=32\n        \"\"\"\n        for arg in args:\n\n            keyval = arg.split('=')\n            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n            key, val = keyval # unpack\n\n            # first translate val into a python object\n            try:\n                val = literal_eval(val)\n                \"\"\"\n                need some explanation here.\n                - if val is simply a string, literal_eval will throw a ValueError\n                - if val represents a thing (like an 3, 3.14, [1,2,3], False, None, etc.) it will get created\n                \"\"\"\n            except ValueError:\n                pass\n\n            # find the appropriate object to insert the attribute into\n            assert key[:2] == '--'\n            key = key[2:] # strip the '--'\n            keys = key.split('.')\n            obj = self\n            for k in keys[:-1]:\n                obj = getattr(obj, k)\n            leaf_key = keys[-1]\n\n            # ensure that this attribute exists\n            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n\n            # overwrite the attribute\n            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n            setattr(obj, leaf_key, val)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T03:58:32.108759Z","iopub.execute_input":"2024-06-17T03:58:32.109129Z","iopub.status.idle":"2024-06-17T03:58:35.471824Z","shell.execute_reply.started":"2024-06-17T03:58:32.109100Z","shell.execute_reply":"2024-06-17T03:58:35.471016Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport math","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:02:40.321598Z","iopub.execute_input":"2024-06-17T04:02:40.322458Z","iopub.status.idle":"2024-06-17T04:02:40.326637Z","shell.execute_reply.started":"2024-06-17T04:02:40.322428Z","shell.execute_reply":"2024-06-17T04:02:40.325596Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class NewGeLU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:02:11.889857Z","iopub.execute_input":"2024-06-17T04:02:11.890600Z","iopub.status.idle":"2024-06-17T04:02:11.895978Z","shell.execute_reply.started":"2024-06-17T04:02:11.890569Z","shell.execute_reply":"2024-06-17T04:02:11.895049Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        assert config.n_embd %config.n_head==0\n        self.c_attn=nn.Linear(config.n_embd,config.n_embd*3)\n        ## 输出层\n        self.c_proj=nn.Linear(config.n_embd,config.n_embd)\n        self.attn_dropout=nn.Dropout(config.attn_pdrop)\n        self.resid_dropout=nn.Dropout(config.resid_pdrop)\n        self.register_buffer('bias',torch.tril(torch.ones(config.block_size,config.block_size))\n                             .view(1,1,config.block_size,config.block_size))\n        self.n_head=config.n_head\n        self.n_embd=config.n_embd\n    def forward(self,x):\n        B,T,C=x.size()\n        q,k,v=self.c_attn(x).split(self.n_embd,dim=2)\n        k=k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n        q=q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n        v=v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n        \n        att=(q@k.transpose(-1,-2))*(1.0/math.sqrt(k.size(-1)))\n        att=att.masked_fill(self.bias[:,:,:T,:T]==0,float('-inf'))\n        att=F.softmax(att,dim=-1)\n        att=self.attn_dropout(att)\n        y=att@v ## B,n,T,T @ B,n,T,nh =B,n,T,nh\n        y=y.transpose(1,2).contiguous().view(B,T,C)\n        \n        ## 加残差连接前dropout\n        y=self.resid_dropout(self.c_proj(y))\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:03:13.676221Z","iopub.execute_input":"2024-06-17T04:03:13.677077Z","iopub.status.idle":"2024-06-17T04:03:13.688481Z","shell.execute_reply.started":"2024-06-17T04:03:13.677042Z","shell.execute_reply":"2024-06-17T04:03:13.687558Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.ln1=nn.LayerNorm(config.n_embd)\n        self.attn=CausalSelfAttention(config)\n        self.ln2=nn.LayerNorm(config.n_embed)\n        self.mlp=nn.ModuleDict(dict(\n            c_fc=nn.Linear(config.n_embd,config.n_embd*4),\n            c_proj=nn.Linear(config.n_embd*4,config.n_embd),\n            act=nn.NewGeLU(),\n            dropout=nn.Dropout(config.resid_pdrop)\n        ))\n        m=self.mlp\n        self.mlpf= lambda x:m.dropout(m.c_proj(m.act(m.c_fc(x))))\n    def forward(self,x):\n        x=x+self.attn(self.ln1(x))\n        x=x+self.mlpf(self.ln2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-17T03:58:46.022899Z","iopub.execute_input":"2024-06-17T03:58:46.023251Z","iopub.status.idle":"2024-06-17T03:58:46.031995Z","shell.execute_reply.started":"2024-06-17T03:58:46.023222Z","shell.execute_reply":"2024-06-17T03:58:46.030841Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class GPT(nn.Module):\n    @staticmethod\n    def get_default_config():\n        C = CN()\n        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n        C.model_type = 'gpt'\n        C.n_layer = None\n        C.n_head = None\n        C.n_embd =  None\n        # these options must be filled in externally\n        C.vocab_size = None\n        C.block_size = None\n        # dropout hyperparameters\n        C.embd_pdrop = 0.1\n        C.resid_pdrop = 0.1\n        C.attn_pdrop = 0.1\n        return C\n    def __init__(self,config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.block_size = config.block_size\n\n        type_given = config.model_type is not None\n        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n        assert type_given ^ params_given # exactly one of these (XOR)\n        if type_given:\n            # translate from model_type to detailed configuration\n            config.merge_from_dict({\n                # names follow the huggingface naming conventions\n                # GPT-1\n                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n                # GPT-2 configs\n                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n                # Gophers\n                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n                # (there are a number more...)\n                # I made these tiny models up\n                'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n            }[config.model_type])\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.embd_pdrop),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters (note we don't count the decoder parameters in lm_head)\n        n_params = sum(p.numel() for p in self.transformer.parameters())\n        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        elif isinstance(module, nn.LayerNorm):\n            torch.nn.init.zeros_(module.bias)\n            torch.nn.init.ones_(module.weight)\n    \n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"\n        Initialize a pretrained GPT model by copying over the weights\n        from a huggingface/transformers checkpoint.\n        \"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n\n        # create a from-scratch initialized minGPT model\n        config = cls.get_default_config()\n        config.model_type = model_type\n        config.vocab_size = 50257 # openai's model vocabulary\n        config.block_size = 1024  # openai's model block_size\n        model = GPT(config)\n        sd = model.state_dict()\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] # ignore these\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\n        # this means that we have to transpose these weights when we import them\n        assert len(keys) == len(sd)\n        for k in keys:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    def configure_optimizers(self, train_config):\n        \"\"\"\n        This long function is unfortunately doing something very simple and is being very defensive:\n        We are separating out all parameters of the model into two buckets: those that will experience\n        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n        We are then returning the PyTorch optimizer object.\n        \"\"\"\n\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        whitelist_weight_modules = (torch.nn.Linear, )\n        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n        for mn, m in self.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n                # random note: because named_modules and named_parameters are recursive\n                # we will see the same tensors p many many times. but doing it this way\n                # allows us to know which parent module any tensor p belongs to...\n                if pn.endswith('bias'):\n                    # all biases will not be decayed\n                    no_decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                    # weights of whitelist modules will be weight decayed\n                    decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                    # weights of blacklist modules will NOT be weight decayed\n                    no_decay.add(fpn)\n\n        # validate that we considered every parameter\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n                                                    % (str(param_dict.keys() - union_params), )\n\n        # create the pytorch optimizer object\n        optim_groups = [\n            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n        ]\n        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n        return optimizer\n    \n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n\n        # if we are given some desired targets also calculate the loss\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n\n        return logits, loss\n    @torch.no_grad()\n    def generator(self,idx,max_new_token,temperature=1.0,do_sample=False,top_k=None):\n        for i in range(max_new_token):\n            idx_cond=idx if idx.size(1)<self.block_size else idx[:,-self.block_size:]\n            logits,_=self(idx_cond)\n            logits=logits[:,-1,:]/temperature\n            if top_k is not None:\n                v,_=torch.top_k(logits,top_k)\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            if do_sample:\n                idx_next = torch.multinomial(probs, num_samples=1)\n            else:\n                _, idx_next = torch.topk(probs, k=1, dim=-1)\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-17T03:58:49.056771Z","iopub.execute_input":"2024-06-17T03:58:49.057149Z","iopub.status.idle":"2024-06-17T03:58:49.097574Z","shell.execute_reply.started":"2024-06-17T03:58:49.057120Z","shell.execute_reply":"2024-06-17T03:58:49.096516Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nSimple training loop; Boilerplate that could apply to any arbitrary neural network,\nso nothing in this file really has anything to do with GPT specifically.\n\"\"\"\n\nimport time\nfrom collections import defaultdict\n\nimport torch\nfrom torch.utils.data.dataloader import DataLoader\n#from mingpt.utils import CfgNode as CN\n\nclass Trainer:\n\n    @staticmethod\n    def get_default_config():\n        C = CN()\n        # device to train on\n        C.device = 'auto'\n        # dataloder parameters\n        C.num_workers = 4\n        # optimizer parameters\n        C.max_iters = None\n        C.batch_size = 64\n        C.learning_rate = 3e-4\n        C.betas = (0.9, 0.95)\n        C.weight_decay = 0.1 # only applied on matmul weights\n        C.grad_norm_clip = 1.0\n        return C\n\n    def __init__(self, config, model, train_dataset):\n        self.config = config\n        self.model = model\n        self.optimizer = None\n        self.train_dataset = train_dataset\n        self.callbacks = defaultdict(list)\n\n        # determine the device we'll train on\n        if config.device == 'auto':\n            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        else:\n            self.device = config.device\n        self.model = self.model.to(self.device)\n        print(\"running on device\", self.device)\n\n        # variables that will be assigned to trainer class later for logging and etc\n        self.iter_num = 0\n        self.iter_time = 0.0\n        self.iter_dt = 0.0\n\n    def add_callback(self, onevent: str, callback):\n        self.callbacks[onevent].append(callback)\n\n    def set_callback(self, onevent: str, callback):\n        self.callbacks[onevent] = [callback]\n\n    def trigger_callbacks(self, onevent: str):\n        for callback in self.callbacks.get(onevent, []):\n            callback(self)\n\n    def run(self):\n        model, config = self.model, self.config\n\n        # setup the optimizer\n        self.optimizer = model.configure_optimizers(config)\n\n        # setup the dataloader\n        train_loader = DataLoader(\n            self.train_dataset,\n            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n            shuffle=False,\n            pin_memory=True,\n            batch_size=config.batch_size,\n            num_workers=config.num_workers,\n        )\n\n        model.train()\n        self.iter_num = 0\n        self.iter_time = time.time()\n        data_iter = iter(train_loader)\n        while True:\n\n            # fetch the next batch (x, y) and re-init iterator if needed\n            try:\n                batch = next(data_iter)\n            except StopIteration:\n                data_iter = iter(train_loader)\n                batch = next(data_iter)\n            batch = [t.to(self.device) for t in batch]\n            x, y = batch\n\n            # forward the model\n            logits, self.loss = model(x, y)\n\n            # backprop and update the parameters\n            model.zero_grad(set_to_none=True)\n            self.loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n            self.optimizer.step()\n\n            self.trigger_callbacks('on_batch_end')\n            self.iter_num += 1\n            tnow = time.time()\n            self.iter_dt = tnow - self.iter_time\n            self.iter_time = tnow\n\n            # termination conditions\n            if config.max_iters is not None and self.iter_num >= config.max_iters:\n                break\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T03:58:56.338208Z","iopub.execute_input":"2024-06-17T03:58:56.338855Z","iopub.status.idle":"2024-06-17T03:58:56.355798Z","shell.execute_reply.started":"2024-06-17T03:58:56.338827Z","shell.execute_reply":"2024-06-17T03:58:56.354796Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nset_seed(3407)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T03:59:01.149714Z","iopub.execute_input":"2024-06-17T03:59:01.150062Z","iopub.status.idle":"2024-06-17T03:59:01.157804Z","shell.execute_reply.started":"2024-06-17T03:59:01.150035Z","shell.execute_reply":"2024-06-17T03:59:01.156842Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nclass SortDataset(Dataset):\n    \"\"\" \n    Dataset for the Sort problem. E.g. for problem length 6:\n    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n    Which will feed into the transformer concatenated as:\n    input:  0 0 2 1 0 1 0 0 0 1 1\n    output: I I I I I 0 0 0 1 1 2\n    where I is \"ignore\", as the transformer is reading the input sequence\n    \"\"\"\n\n    def __init__(self, split, length=6, num_digits=3):\n        assert split in {'train', 'test'}\n        self.split = split\n        self.length = length\n        self.num_digits = num_digits\n    \n    def __len__(self):\n        return 10000 # ...\n    \n    def get_vocab_size(self):\n        return self.num_digits\n    \n    def get_block_size(self):\n        # the length of the sequence that will feed into transformer, \n        # containing concatenated input and the output, but -1 because\n        # the transformer starts making predictions at the last input element\n        return self.length * 2 - 1\n\n    def __getitem__(self, idx):\n        \n        # use rejection sampling to generate an input example from the desired split\n        while True:\n            # generate some random integers\n            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n            # half of the time let's try to boost the number of examples that \n            # have a large number of repeats, as this is what the model seems to struggle\n            # with later in training, and they are kind of rate\n            if torch.rand(1).item() < 0.5:\n                if inp.unique().nelement() > self.length // 2:\n                    # too many unqiue digits, re-sample\n                    continue\n            # figure out if this generated example is train or test based on its hash\n            h = hash(pickle.dumps(inp.tolist()))\n            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n            if inp_split == self.split:\n                break # ok\n        \n        # solve the task: i.e. sort\n        sol = torch.sort(inp)[0]\n\n        # concatenate the problem specification and the solution\n        cat = torch.cat((inp, sol), dim=0)\n\n        # the inputs to the transformer will be the offset sequence\n        x = cat[:-1].clone()\n        y = cat[1:].clone()\n        # we only want to predict at output locations, mask out the loss at the input locations\n        y[:self.length-1] = -1\n        return x, y\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T03:59:03.297768Z","iopub.execute_input":"2024-06-17T03:59:03.298545Z","iopub.status.idle":"2024-06-17T03:59:03.309051Z","shell.execute_reply.started":"2024-06-17T03:59:03.298515Z","shell.execute_reply":"2024-06-17T03:59:03.308019Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# print an example instance of the dataset\ntrain_dataset = SortDataset('train',length=10, num_digits=10)\ntest_dataset = SortDataset('test',length=10, num_digits=10)\nx, y = train_dataset[0]\nprint(x,y)\nfor a, b in zip(x,y):\n    print(int(a),int(b))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:08:55.364932Z","iopub.execute_input":"2024-06-17T04:08:55.365285Z","iopub.status.idle":"2024-06-17T04:08:55.373329Z","shell.execute_reply.started":"2024-06-17T04:08:55.365258Z","shell.execute_reply":"2024-06-17T04:08:55.372422Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor([9, 3, 9, 4, 5, 4, 5, 7, 9, 9, 3, 4, 4, 5, 5, 7, 9, 9, 9]) tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1,  3,  4,  4,  5,  5,  7,  9,  9,  9,\n         9])\n9 -1\n3 -1\n9 -1\n4 -1\n5 -1\n4 -1\n5 -1\n7 -1\n9 -1\n9 3\n3 4\n4 4\n4 5\n5 5\n5 7\n7 9\n9 9\n9 9\n9 9\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a GPT instance\nmodel_config = GPT.get_default_config()\nmodel_config.model_type = 'gpt-nano'\nmodel_config.vocab_size = train_dataset.get_vocab_size()\nmodel_config.block_size = train_dataset.get_block_size()\nmodel = GPT(model_config)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:09:02.515287Z","iopub.execute_input":"2024-06-17T04:09:02.515667Z","iopub.status.idle":"2024-06-17T04:09:02.529796Z","shell.execute_reply.started":"2024-06-17T04:09:02.515637Z","shell.execute_reply":"2024-06-17T04:09:02.529046Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"number of parameters: 0.09M\n","output_type":"stream"}]},{"cell_type":"code","source":"train_config = Trainer.get_default_config()\ntrain_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\ntrain_config.max_iters = 5000\ntrain_config.num_workers = 0\ntrainer = Trainer(train_config, model, train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:09:11.051456Z","iopub.execute_input":"2024-06-17T04:09:11.052084Z","iopub.status.idle":"2024-06-17T04:09:11.059657Z","shell.execute_reply.started":"2024-06-17T04:09:11.052054Z","shell.execute_reply":"2024-06-17T04:09:11.058757Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"running on device cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"def batch_end_callback(trainer):\n    if trainer.iter_num % 100 == 0:\n        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\ntrainer.set_callback('on_batch_end', batch_end_callback)\n\ntrainer.run()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:09:13.064909Z","iopub.execute_input":"2024-06-17T04:09:13.065754Z","iopub.status.idle":"2024-06-17T04:10:25.810118Z","shell.execute_reply.started":"2024-06-17T04:09:13.065719Z","shell.execute_reply":"2024-06-17T04:10:25.809355Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"iter_dt 0.00ms; iter 0: train loss 2.32167\niter_dt 14.68ms; iter 100: train loss 1.01312\niter_dt 14.45ms; iter 200: train loss 0.45703\niter_dt 14.65ms; iter 300: train loss 0.28763\niter_dt 14.31ms; iter 400: train loss 0.23086\niter_dt 14.19ms; iter 500: train loss 0.18659\niter_dt 14.95ms; iter 600: train loss 0.16426\niter_dt 15.54ms; iter 700: train loss 0.12000\niter_dt 14.86ms; iter 800: train loss 0.18479\niter_dt 14.72ms; iter 900: train loss 0.11591\niter_dt 13.77ms; iter 1000: train loss 0.08665\niter_dt 17.93ms; iter 1100: train loss 0.07709\niter_dt 14.17ms; iter 1200: train loss 0.10342\niter_dt 14.91ms; iter 1300: train loss 0.08778\niter_dt 14.43ms; iter 1400: train loss 0.06175\niter_dt 14.10ms; iter 1500: train loss 0.08928\niter_dt 14.37ms; iter 1600: train loss 0.11070\niter_dt 16.37ms; iter 1700: train loss 0.04550\niter_dt 14.22ms; iter 1800: train loss 0.08580\niter_dt 14.10ms; iter 1900: train loss 0.06832\niter_dt 14.35ms; iter 2000: train loss 0.07252\niter_dt 14.31ms; iter 2100: train loss 0.06269\niter_dt 14.61ms; iter 2200: train loss 0.04959\niter_dt 14.33ms; iter 2300: train loss 0.03939\niter_dt 14.66ms; iter 2400: train loss 0.07794\niter_dt 14.11ms; iter 2500: train loss 0.04872\niter_dt 14.32ms; iter 2600: train loss 0.04871\niter_dt 16.71ms; iter 2700: train loss 0.05167\niter_dt 14.53ms; iter 2800: train loss 0.05390\niter_dt 15.78ms; iter 2900: train loss 0.05633\niter_dt 14.52ms; iter 3000: train loss 0.04292\niter_dt 14.62ms; iter 3100: train loss 0.03358\niter_dt 14.11ms; iter 3200: train loss 0.06765\niter_dt 16.66ms; iter 3300: train loss 0.02842\niter_dt 13.88ms; iter 3400: train loss 0.03809\niter_dt 14.36ms; iter 3500: train loss 0.02944\niter_dt 13.52ms; iter 3600: train loss 0.02622\niter_dt 14.53ms; iter 3700: train loss 0.06814\niter_dt 14.44ms; iter 3800: train loss 0.03972\niter_dt 14.27ms; iter 3900: train loss 0.06789\niter_dt 15.03ms; iter 4000: train loss 0.03302\niter_dt 14.55ms; iter 4100: train loss 0.04203\niter_dt 14.48ms; iter 4200: train loss 0.05522\niter_dt 14.20ms; iter 4300: train loss 0.02860\niter_dt 13.89ms; iter 4400: train loss 0.02219\niter_dt 14.43ms; iter 4500: train loss 0.04570\niter_dt 13.89ms; iter 4600: train loss 0.04355\niter_dt 13.97ms; iter 4700: train loss 0.03386\niter_dt 15.63ms; iter 4800: train loss 0.04003\niter_dt 15.33ms; iter 4900: train loss 0.01899\n","output_type":"stream"}]},{"cell_type":"code","source":"# now let's perform some evaluation\nmodel.eval();","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:10:28.568184Z","iopub.execute_input":"2024-06-17T04:10:28.569003Z","iopub.status.idle":"2024-06-17T04:10:28.573328Z","shell.execute_reply.started":"2024-06-17T04:10:28.568968Z","shell.execute_reply":"2024-06-17T04:10:28.572399Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def eval_split(trainer, split, max_batches):\n    dataset = {'train':train_dataset, 'test':test_dataset}[split]\n    n = train_dataset.length # naugy direct access shrug\n    results = []\n    mistakes_printed_already = 0\n    loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)\n    for b, (x, y) in enumerate(loader):\n        x = x.to(trainer.device)\n        y = y.to(trainer.device)\n        # isolate the input pattern alone\n        inp = x[:, :n]\n        sol = y[:, -n:]\n        # let the model sample the rest of the sequence\n        cat = model.generator(inp, n, do_sample=False) # using greedy argmax, not sampling\n        sol_candidate = cat[:, n:] # isolate the filled in sequence\n        # compare the predicted sequence to the true sequence\n        correct = (sol == sol_candidate).all(1).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line haha\n        for i in range(x.size(0)):\n            results.append(int(correct[i]))\n            if not correct[i] and mistakes_printed_already < 3: # only print up to 5 mistakes to get a sense\n                mistakes_printed_already += 1\n                print(\"GPT claims that %s sorted is %s but gt is %s\" % (inp[i].tolist(), sol_candidate[i].tolist(), sol[i].tolist()))\n        if max_batches is not None and b+1 >= max_batches:\n            break\n    rt = torch.tensor(results, dtype=torch.float)\n    print(\"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean()))\n    return rt.sum()\n\n# run a lot of examples from both train and test through the model and verify the output correctness\nwith torch.no_grad():\n    train_score = eval_split(trainer, 'train', max_batches=50)\n    test_score  = eval_split(trainer, 'test',  max_batches=50)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:10:34.328489Z","iopub.execute_input":"2024-06-17T04:10:34.328855Z","iopub.status.idle":"2024-06-17T04:10:38.211640Z","shell.execute_reply.started":"2024-06-17T04:10:34.328828Z","shell.execute_reply":"2024-06-17T04:10:38.210744Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"GPT claims that [9, 5, 6, 5, 6, 6, 4, 6, 5, 1] sorted is [1, 4, 5, 5, 5, 5, 6, 6, 6, 9] but gt is [1, 4, 5, 5, 5, 6, 6, 6, 6, 9]\nGPT claims that [8, 9, 9, 7, 3, 3, 9, 9, 9, 4] sorted is [3, 3, 4, 7, 8, 8, 9, 9, 9, 9] but gt is [3, 3, 4, 7, 8, 9, 9, 9, 9, 9]\nGPT claims that [3, 8, 2, 8, 8, 8, 9, 9, 9, 8] sorted is [2, 3, 8, 8, 8, 8, 8, 8, 9, 9] but gt is [2, 3, 8, 8, 8, 8, 8, 9, 9, 9]\ntrain final score: 4952/5000 = 99.04% correct\nGPT claims that [7, 4, 4, 7, 7, 9, 3, 3, 3, 7] sorted is [3, 3, 3, 4, 4, 4, 7, 7, 7, 9] but gt is [3, 3, 3, 4, 4, 7, 7, 7, 7, 9]\nGPT claims that [4, 4, 2, 5, 9, 5, 5, 2, 3, 5] sorted is [2, 2, 3, 4, 4, 4, 5, 5, 5, 9] but gt is [2, 2, 3, 4, 4, 5, 5, 5, 5, 9]\nGPT claims that [9, 7, 9, 7, 2, 5, 7, 4, 9, 7] sorted is [2, 4, 5, 7, 7, 7, 7, 8, 9, 9] but gt is [2, 4, 5, 7, 7, 7, 7, 9, 9, 9]\ntest final score: 4937/5000 = 98.74% correct\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's run a random given sequence through the model as well\nn = train_dataset.length # naugy direct access shrug\ninp = torch.tensor([[9, 5, 6, 5, 6, 5, 4, 6, 5, 1]], dtype=torch.long).to(trainer.device)\nassert inp[0].nelement() == n\nwith torch.no_grad():\n    cat = model.generator(inp, n, do_sample=False)\nsol = torch.sort(inp[0])[0]\nsol_candidate = cat[:, n:]\nprint('input sequence  :', inp.tolist())\nprint('predicted sorted:', sol_candidate.tolist())\nprint('gt sort         :', sol.tolist())\nprint('matches         :', bool((sol == sol_candidate).all()))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T04:11:36.945532Z","iopub.execute_input":"2024-06-17T04:11:36.945871Z","iopub.status.idle":"2024-06-17T04:11:36.982329Z","shell.execute_reply.started":"2024-06-17T04:11:36.945847Z","shell.execute_reply":"2024-06-17T04:11:36.981429Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"input sequence  : [[9, 5, 6, 5, 6, 5, 4, 6, 5, 1]]\npredicted sorted: [[1, 4, 5, 5, 5, 5, 6, 6, 6, 9]]\ngt sort         : [1, 4, 5, 5, 5, 5, 6, 6, 6, 9]\nmatches         : True\n","output_type":"stream"}]}]}
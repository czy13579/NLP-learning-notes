## bert作用
### 能基于上下文进行词嵌入

## bert模型架构
**叠加transformer编码器。**
### 嵌入层：
- 词嵌入层：将词汇转换为向量表示。
- 位置嵌入层：将位置信息编码到向量表示中。
- 分段嵌入层：将分段信息编码到向量表示中。
bert的嵌入层由以上三个嵌入层相加得到

### tokenizer:
- WordPieces

### 预训练策略
  - **掩码语言模型（MLM）**：随机替换输入序列中的一小部分词，并预测被替换的词。
  - **下一句预测（NSP）**：预测第二个句子是否是第一个句子的下句。
  
### 其他细节
 - **数据配比**：随机掩盖15%的标记，采用80-10-10规则，80%情况下使用[MASK]替代这个标志，10%情况下使用随机词替换，10%情况下保持原样。
 - **全词掩码**：某个子词被掩蔽后，整个词都被掩蔽。


## 模型优点
自动回归式语言模型本质上只能从一个方向阅读句子，而bert模型可以从两个方向阅读句子，从而更好地理解上下文。

## bert输出
- last_hidden_state：最后一层编码器每个词的向量表示。
- pooler_output：最后一层编码器[CLS]位置的向量表示。即句子的向量表示。
- hidden_states：bert的每一层的输出。